# -*- coding: utf-8 -*-
"""26_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YCIxSSo-CNpvRKdo58Wi19ehWl9JhNYX
"""

import pandas as pd
import numpy as np
from xgboost.sklearn import XGBRegressor
from google.colab import drive
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression, SGDRegressor, BayesianRidge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
import pickle

drive.mount('/content/drive')

"""# **Importing Data**"""

path_21 = '/content/drive/My Drive/Colab Notebooks/datasets/players_21.csv'
path_22 = '/content/drive/My Drive/Colab Notebooks/datasets/players_22.csv'

with open(path_21) as dataset_21, open(path_22) as dataset_22:
    fifa_21 = pd.read_csv(path_21)
    fifa_22 = pd.read_csv(path_22)

print("\nFIFA 21 Dataset Shape: " + str(fifa_21.shape))
print("FIFA 22 Dataset Shape: " + str(fifa_22.shape) + "\n")

drop = []
for col in fifa_21.columns:
    missing = (len(fifa_21[col]) - fifa_21[col].count())/len(fifa_21[col]) * 100
    if missing > 30:
        print(f'{col} = {round(missing, 2)}%')
        drop.append(col)

fifa_21.drop(columns = drop, inplace = True)
fifa_22.drop(columns = drop, inplace = True)

print("\nFIFA 21 Dataset Shape: " + str(fifa_21.shape))
print("FIFA 22 Dataset Shape: " + str(fifa_22.shape) + "\n")

"""We are dropping all the columns that have more than 30% of their values missing because imputing them could create a bias.

# **Exploratory Data Analysis**
"""

import matplotlib.pyplot as plt

age = fifa_21['age']
bins=np.arange(0, 60, 1)
pd.cut(age, bins=bins, labels=bins[1:]).astype(int).value_counts().sort_index().plot.bar(width=1, figsize=(16,10))
plt.xlabel('Ages')
plt.xticks(rotation=90)
plt.title('Player Age Distribution')

fifa_21.plot(kind="scatter", x="overall", y="wage_eur", figsize=(19,6))

fifa_21.plot(kind="scatter", x="overall", y="value_eur", figsize=(19,6))

"""# **Subsetting Data**"""

ovr_21 = fifa_21[['overall']]
ovr_22 = fifa_22[['overall']]
correlate = fifa_21.corr()['overall'].sort_values(ascending=False)
keys = correlate.keys()
values = correlate.values
keys_to_use = []
for key,val in zip(keys, values):
    if(val>0.4): # 0.1
        keys_to_use.append(key)
keys_to_use.remove('potential')
keys_to_use.remove('release_clause_eur')
keys_to_use.remove('wage_eur')
keys_to_use.remove('value_eur')
keys_to_use.remove('international_reputation')

train_21 = fifa_21[keys_to_use]
train_22 = fifa_22[keys_to_use]
ovr_21 = train_21['overall']
ovr_22 = train_22['overall']

"""Imputation"""

imp = SimpleImputer()
imp.fit(train_21)
imputed_data=imp.fit_transform(train_21)
train_21_imp=pd.DataFrame(imputed_data, columns=train_21.columns)

imputed_data = imp.fit_transform(train_22)
train_22_imp=pd.DataFrame(imputed_data, columns=train_22.columns)

"""Encoding"""

categorical = ['work_rate']
labels_21 = fifa_21[categorical]
labels_22 = fifa_22[categorical]
encoder = LabelEncoder()
labels_21 = pd.DataFrame(encoder.fit_transform(labels_21), columns = labels_21.columns)
labels_22 = pd.DataFrame(encoder.fit_transform(labels_22), columns = labels_22.columns)

"""Scaling"""

train_21_imp.drop(columns='overall', inplace=True)
train_22_imp.drop(columns='overall', inplace=True)

sc = StandardScaler()
scaled = sc.fit_transform(train_21_imp)
train_21 = pd.DataFrame(scaled, columns = train_21_imp.columns)

scaled = sc.fit_transform(train_22_imp)
train_22 = pd.DataFrame(scaled, columns = train_22_imp.columns)

"""Randomise the arrangement of the rows in each dataframe"""

train_21 = pd.concat([train_21, ovr_21], axis = 1).sample(frac=1)
train_22 = pd.concat([train_22, ovr_22], axis = 1).sample(frac=1)



ovr_21 = train_21['overall']
ovr_22 = train_22['overall']
train_21.drop(columns='overall', inplace=True)
train_22.drop(columns='overall', inplace=True)



"""# **Model Training**"""

# imputed_data=imp.fit_transform(train_21)
# train_21=pd.DataFrame(imputed_data, columns=train_21.columns)

# imputed_data = imp.fit_transform(train_22)
# train_22=pd.DataFrame(imputed_data, columns=train_22.columns)

rf = RandomForestRegressor(random_state=42, n_jobs=-1)
xgb_reg = XGBRegressor(n_estimators = 1000, verbosity = 1,max_depth = 5, gamma = 0.2, subsample = 0.2, learning_rate = 0.3)
g_reg = GradientBoostingRegressor(random_state=42)
sv = SVR()
vc = VotingRegressor(estimators=[('random_forest', rf), ('xgb', g_reg), ('svm', sv)])

for reg in (rf, xgb_reg, g_reg, sv, vc):
    reg.fit(train_21, ovr_21)
    y_pred = reg.predict(train_21)
    mse = mean_squared_error(ovr_21, y_pred)
    mae = mean_absolute_error(ovr_21, y_pred)
    r2 = r2_score(ovr_21, y_pred)
    print(f"The {reg.__class__.__name__} model had {round(mse, 4)} MSE, {round(mae, 4)} MAE and {round(r2, 4)} R2 Score")

for reg in (rf, xgb_reg, g_reg, sv, vc):
    Xtrain,Xtest,Ytrain,Ytest=train_test_split(train_21,ovr_21,test_size=0.2,random_state=42)
    reg.fit(Xtrain, Ytrain)
    y_pred = reg.predict(Xtest)
    mse = mean_squared_error(y_pred, Ytest)
    mae = mean_absolute_error(y_pred, Ytest)
    r2 = r2_score(y_pred, Ytest)
    print(f"The {reg.__class__.__name__} model had {round(mse, 4)} MSE, {round(mae, 4)} MAE and {round(r2, 4)} R2 Score")

for reg in (rf, xgb_reg, g_reg, sv, vc):
    y_pred = reg.predict(train_22)
    mse = mean_squared_error(ovr_22, y_pred)
    mae = mean_absolute_error(ovr_22, y_pred)
    r2 = r2_score(ovr_22, y_pred)
    print(f"The {reg.__class__.__name__} model had {round(mse, 4)} MSE, {round(mae, 4)} MAE and {round(r2, 4)} R2 Score")

from sklearn.model_selection import KFold,GridSearchCV

cv=KFold(n_splits=3)
param_grid = [
{'bootstrap': [True], 'n_estimators': [10, 30 ,50, 100]},
{'bootstrap': [False], 'n_estimators': [10, 30 ,50, 100]},
]
grid_search = GridSearchCV(rf, param_grid, cv=5,
scoring='neg_mean_squared_error',
return_train_score=True)
grid_search.fit(train_21, ovr_21)

grid_search.best_params_

model = grid_search.best_estimator_

y_pred = model.predict(train_22)

model.score(train_22, ovr_22)

mean_squared_error(ovr_22, y_pred)

mean_absolute_error(ovr_22, y_pred)

with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)

with open('scaler.pkl', 'wb') as file:
    pickle.dump(sc, file)

y_pred



